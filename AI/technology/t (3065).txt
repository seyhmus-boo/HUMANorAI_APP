Good morning. Today's lecture addresses the experimental challenges inherent in researching Artificial Intelligence (AI) and its subset, Machine Learning (ML).  A primary hurdle lies in data acquisition and quality.  ML algorithms are heavily reliant on large, clean, and representative datasets. Obtaining such datasets can be prohibitively expensive and time-consuming, particularly in niche areas. Furthermore, biases embedded within datasets can propagate through the model, leading to unfair or discriminatory outcomes â€“ a significant ethical challenge.

Another key difficulty resides in evaluating model performance.  Traditional metrics, while useful, often fail to capture the nuances of real-world applications.  Evaluating generalisation performance, particularly in scenarios with limited or unseen data, remains a significant open problem.  The "black box" nature of many deep learning models also complicates interpretability and debugging, hindering our understanding of why a model makes specific decisions.  Finally, the rapid evolution of AI necessitates ongoing methodological advancements to keep pace with technological progress and address emerging challenges.  Overcoming these experimental obstacles is crucial for the responsible and effective development of AI systems.