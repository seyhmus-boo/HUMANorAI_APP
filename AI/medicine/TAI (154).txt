Artificial intelligence (AI) holds transformative potential for healthcare diagnostics, promising improved accuracy, speed, and accessibility.  However, realizing this potential faces significant experimental challenges. This presentation will explore key limitations in the application of AI to diagnostic tasks, focusing on three critical areas. First, the inherent bias in training data, often reflecting existing healthcare disparities, leads to algorithmic inaccuracies and potentially exacerbates health inequalities.  Second, the “black box” nature of many AI models hinders interpretability and trust, making it difficult to validate diagnoses and establish clinical confidence.  Finally, the heterogeneity of medical data, encompassing diverse imaging modalities, patient populations, and clinical presentations, poses significant difficulties in developing robust and generalizable AI diagnostic tools.  We will discuss ongoing efforts to address these challenges, including the development of explainable AI (XAI) techniques, strategies for mitigating bias through data augmentation and re-weighting, and the exploration of federated learning for improved data privacy and generalizability.  The presentation will conclude by emphasizing the need for rigorous experimental design and validation to ensure the safe and equitable deployment of AI in healthcare diagnostics.